{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b618988-4269-4d5a-9823-a92606616930",
   "metadata": {},
   "source": [
    "## Initializing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff12084-5987-46f2-afbb-172ac972cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/shahayush954/spark-3.4.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486bc778-d312-4c43-8ff0-c44499314d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 23:36:44 WARN Utils: Your hostname, ubuntu-22 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/08/21 23:36:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/21 23:36:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp_tools').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a1ae3-4262-4592-a990-d641f7594e69",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bde012d-8dde-475f-9304-37255d10193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9257dc-096b-4158-a13f-2c57d1ff5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d30d3ea-3670-4f07-807c-1aa73e4a4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,Regression,models,are,neat')\n",
    "], ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b63b1a-9dcf-4b52-b81b-a25654702097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,Regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentence_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03cf4f8-b504-4413-970a-b2013640da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "regex_tokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecdefae-d47b-4208-a4d3-1202e3409840",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda words:len(words), IntegerType())  ## create a user defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b62ecd8a-8f1c-47c1-9f1e-d8767fd5a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbb2f19-7997-443a-9dbb-8c3bbb09c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,Regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbb2aea-a204-4e5b-ac54-4cbaf52f8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,Regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a143b533-b7b9-4481-9ac8-510142d527a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tokenized = regex_tokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4620e9d-342b-40fa-bb46-fdd4b4ec6bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,Regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rg_tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d176a-f406-4af9-8e7f-16ac581014c4",
   "metadata": {},
   "source": [
    "## Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa8ef30-8790-4ee2-b169-062a6e93c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b26366c-c18b-496c-aa26-f1d0c4aacdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenized_df = spark.createDataFrame([\n",
    "    (0, ['I', 'saw', 'the', 'green', 'horse']),\n",
    "    (1, ['Mary', 'had', 'a', 'little', 'lamb'])\n",
    "],['id','tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16830bc7-7b71-48dd-b2d2-1b397e44e6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              tokens|\n",
      "+---+--------------------+\n",
      "|  0|[I, saw, the, gre...|\n",
      "|  1|[Mary, had, a, li...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d47b5052-cf75-463c-914c-a867917ea8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='tokens', outputCol='stop_words_removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56af510b-a94b-4da6-ad3a-5ce48d3720ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              tokens|  stop_words_removed|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, gre...| [saw, green, horse]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentence_tokenized_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65336219-d5c4-48eb-b62d-a9197db09a5b",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27537adb-d82f-40f5-a135-1381c41f8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5697fba-8df7-4e47-a559-385ce25a2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, ['Hi','I','heard','about','Spark']),\n",
    "    (1, ['I','wish','Java','could','use','case','classes']),\n",
    "    (2, ['Logistic','Regression','models','are','neat'])\n",
    "], ['id', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3993619d-652f-4628-9f8f-2ed6dade8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "nGram = NGram(n=2, inputCol='words', outputCol='grams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809ed33c-81de-4f95-bf3a-dbf5aaea9ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|grams                                                             |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic Regression, Regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nGram.transform(wordDataFrame).select('grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578a635-1368-41cc-8b63-252a65803221",
   "metadata": {},
   "source": [
    "## Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "661752de-ff57-4259-b6cb-1f7a10e3f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01079810-d576-43c6-8e5e-947f8b16d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish Java could use case classes'),\n",
    "    (2, 'Logistic,Regression,models,are,neat')\n",
    "], ['label', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a166d76c-3fb5-41a3-ae32-a6592bdaace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|Hi I heard about ...|\n",
      "|    1|I wish Java could...|\n",
      "|    2|Logistic,Regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sentence_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7581e72d-81bb-4b5e-9dbb-7d9bcf0af34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3457c015-395a-4817-92b4-c89a55bef948",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = tokenizer.transform(sentence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063ed8ea-2e5f-4332-a86f-3ea174370eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|    1|I wish Java could...|[i, wish, java, c...|\n",
      "|    2|Logistic,Regressi...|[logistic,regress...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1899ebad-b54b-4dc9-ba58-8a36b5e24fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')\n",
    "featurized_data = hashing_tf.transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbc8b6a-6d6f-41a7-bb99-8bdf696c7ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idf_model = idf.fit(featurized_data)\n",
    "idf_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8dcf90-c46f-4042-9ddf-edc81063b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 23:48:51 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/08/21 23:48:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|Hi I heard about ...|[hi, i, heard, ab...|(262144,[18700,19...|(262144,[18700,19...|\n",
      "|    1|I wish Java could...|[i, wish, java, c...|(262144,[19036,20...|(262144,[19036,20...|\n",
      "|    2|Logistic,Regressi...|[logistic,regress...|(262144,[11534],[...|(262144,[11534],[...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b6917-342b-4181-a85e-b46d571f5fea",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f36fe728-a88f-4e46-be79-5188f3e13f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3049697b-345e-4ce5-a42d-f844aeb31e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], ['id', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f14b4cce-8ab0-4e84-9466-ab92ca5028c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a809dfef-2f4d-421f-b7d5-9b5b6cdd6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46251784-4628-4c61-907f-2f3776241d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cv_model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "004be623-1ecc-4d98-b246-64d5a535ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cv_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55b764c9-91e3-479a-aa2b-e3c99d579532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7d020-cc52-40cf-a4c3-c9dd885e48c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
